# -*- coding: utf-8 -*-
"""VGG16-final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O88B9kcbBlEsJ9XcAuYebR8lFiK9-vse
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'kermany2018:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F17839%2F23942%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T122629Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dcf31ca251717b3d1cdcfcd6fb0bbeab108fc3a84c6a4270b99978dcbfd965e5d1560453bcc53b70c45c3213dcb21b482a0fa26b8831ef3304b6768d18eacc6a245a896ee8c21c991bf75287fe083638f7da69964b8a348e7909de9b4d41c4379b89b9961de10e5f1d04eddb550edf32fb2c2a4f18edcdb93ef87c3b448efc09650382c08447f0d7e6c19fd6ea7a4f73ea51f282df45ff55529b8f4aed29f486376cb3ede0b0f147c5a521769b06b983c4042c98cefa7d92ed7ca58ae9f6c009775133271b8e6ab24da08b593697e0c31079f1e529a1b9cac5f7a10f0d28cc123cf42d6026d71ba713af96462fba8141424b2e26d90db86953fcec66e0eadae0b,vgg16/keras/vgg19/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F24793%2F29426%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T122629Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3De0ca0b9461c2d5c3302d23d707e1d33c16f2e4b3e379ce3ed0df244209575fd9a5baaddec9af45c2126add072c9b43ef6db05a06639adbff542f1be5b370c731fcc2cb69e1872b5b7dcefe3a0a28598e1f79a17f46b2ee10045a76516f248897283971f05d72234a6c994349b95e37ecd42fa342abcc4bafb0f04fae2166f347ec728943f34161322d145e89431f53856bae8586ee9185a260a163d93d7476b40d84439ef24f47da52217862e991174d5b49ce3ef353865748bf35cb49b0dee25fa335aa8de587efc13fba923abe979f58e43518da6f0283f11db9302bf316f086904435331c42ec2950360899f97c8e0ad826a75d74113738833815c5decb3f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import InceptionResNetV2
import numpy as np
from glob import glob
import matplotlib.pyplot as plt
# re-size all the images to this
IMAGE_SIZE = [224, 224]

inception_resnet =VGG16(input_shape=IMAGE_SIZE + [3],
    weights='imagenet',
    include_top=False,
    )

# Freeze the weights of the pre-trained layers
for layer in inception_resnet.layers:
    layer.trainable = False

x = Flatten()(inception_resnet.output)
prediction = Dense(units=4, activation='softmax')(x)

# create a model object

model = Model(inputs=inception_resnet.input, outputs=prediction)

model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)

training_data_processor = ImageDataGenerator(rescale=1./255,fill_mode='nearest')

test_data_processor = ImageDataGenerator(rescale = 1./255,fill_mode='nearest')

train_path = '/kaggle/input/kermany2018/OCT2017 /train'
test_path = '/kaggle/input/kermany2018/OCT2017 /test'
val_path = '/kaggle/input/kermany2018/OCT2017 /val'

# Load data into Python
training_data = training_data_processor.flow_from_directory(
    train_path,
    target_size = (224, 224),
    batch_size = 32,
    class_mode = 'categorical')

testing_data = test_data_processor.flow_from_directory(
    test_path,
    target_size = (224 ,224),
    batch_size = 32,
    class_mode = 'categorical',
    shuffle = False)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=7, verbose=1)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(class_weight='balanced', classes = np.unique(training_data.classes), y= training_data.classes)
class_weight_dict = dict(enumerate(class_weights))

history = model.fit(
    training_data,
    epochs=15,
    steps_per_epoch=327,
    validation_data=testing_data,
    callbacks=[early_stopping],
    class_weight=class_weight_dict
)

predictions = model.predict(testing_data)

test_loss, test_accuracy = model.evaluate(testing_data)

y_pred = np.argmax(predictions, axis=1)
y_true = testing_data.classes

labels = {value: key for key, value in training_data.class_indices.items()}

print("Label Mappings for classes present in the training and validation datasets\n")
for key, value in labels.items():
    print(f"{key} : {value}")

from sklearn.metrics import confusion_matrix, classification_report
cf_mtx = confusion_matrix(y_true, y_pred)

print(classification_report(y_true, y_pred, target_names=labels.values()))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

ax= plt.subplot()
sns.heatmap(cf_mtx, annot=True,cmap='Blues',ax=ax,fmt='g')
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['CNV', 'DME','DRUSEN','NORMAL']); ax.yaxis.set_ticklabels(['CNV', 'DME','DRUSEN','NORMAL']);

print(os.listdir(train_path))
print(os.listdir(test_path))
print(os.listdir(val_path))

subfolders = os.listdir(train_path)

plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')

import datetime

str_time = datetime.datetime.now().strftime("%m-%d %H:%M:%S")
model_path = '/kaggle/working/' + str_time + 'model.h5'
model.save(model_path)

fig, axes = plt.subplots(4, 5, figsize=(15, 8))
axes = axes.flatten()
axes_counter = 0

# Loop through each subfolder
for subfolder in subfolders:
    # extract the files in subfolder
    files = os.listdir(os.path.join(test_path, subfolder))

    # Select any 5 files from the subfolder
    selected_files = files[25:30]

    # Loop through the selected files and plot them in the corresponding subplot
    for i, image in enumerate(selected_files):
        i += axes_counter
        image_path = os.path.join(test_path, subfolder, image)
        img = plt.imread(image_path)
        axes[i].imshow(img)
        axes[i].axis('off')
        axes[i].set_title(subfolder)

    axes_counter +=5

plt.tight_layout()
plt.show()

